import music21
from tqdm import tqdm
from src.prep_batch_loading import read_pitchnames, load_notes, load_song, read_size_of_data
import numpy as np
from music21 import converter, instrument, note, chord
from src.train_notes import create_model


def sample(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)


def generate_music(l_model, input_file, output_file, length, temperature=0.2):
    # Get model information
    pitchnames = read_pitchnames()
    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))
    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))
    n_vocab = len(pitchnames)  # get amount of pitch names
    sequence_length = read_size_of_data()[0]

    # Get seed music from file
    seed, time_step = load_song(input_file)
    if not seed:
        raise ValueError("Empty song")
    # Check if we can run on the song
    cannot_parse = any([True for item in seed if item not in pitchnames])
    if cannot_parse:
        raise ValueError("Song contains notes or chords that is model was not trained to handle")

    # Format song for network
    network_seed = []
    # Pad array with zeros
    for i in range(sequence_length - len(seed)):
        network_seed.append(0)
    # Add notes
    for i in range(max(len(seed) - sequence_length, 0), len(seed)):
        network_seed.append(note_to_int[seed[i]])

    # Generate notes
    pattern = network_seed
    prediction_output = []
    extending_length = length-(len(seed)*time_step)
    time_step *= 1.5
    for note_index in tqdm(range(int(round(extending_length/time_step))), desc='Generating music'):
        prediction_input = np.reshape(pattern, (1, sequence_length))    # Shape input for network
        prediction = l_model.predict(prediction_input, verbose=0)       # Predict
        prediction = prediction.flatten()                               # Flatten prediction to 1D
        index = sample(prediction, temperature)                         # Use some randomness to avoid repetition
        result = int_to_note[index]                                     # get note format
        prediction_output.append(result)                                # append results
        pattern.append(index)
        pattern = pattern[1:sequence_length + 1]

    # Seed output notes with seed
    midi = converter.parse(input_file)
    generated_part = music21.stream.Part()
    offset = midi.quarterLength
    # create note and chord objects based on the values generated by the model
    for section in prediction_output:
        # section is a chord
        if ('.' in section) or section.isdigit():
            notes_in_chord = section.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = music21.note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            generated_part.insert(offset, new_chord)
        # section is a note
        else:
            new_note = music21.note.Note(section)
            new_note.storedInstrument = instrument.Piano()
            generated_part.insert(offset, new_note)
        # increase offset each iteration so that notes do not stack
        offset += time_step
    midi.insert(0, generated_part)
    midi.write('midi', fp=output_file)
    midi.show('text')


if __name__ == "__main__":
    # load in model
    n_vocab = len(read_pitchnames())  # get amount of pitch names
    model = create_model(read_size_of_data(), n_vocab)
    model.summary()
    model.load_weights("../files/models/notes/model-13-4.7501.hdf5")

    # Generate music
    generate_music(model, "../files/seeds/bach_846.mid", "../files/|Asongs/bach_1.mid", 60)
