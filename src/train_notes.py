import glob

from keras import Sequential
from keras.callbacks import ModelCheckpoint
from keras.layers import Activation, Dense, Dropout, LSTM, Embedding

from include.Generator import My_Custom_Generator
from include.plot_losses import PlotLearning
from include.user_input import get_user_options, get_user_yes_no, \
    get_user_non_negative_number, get_user_filename

from src.prep_batch_loading import read_size_of_data, read_pitchnames


def create_model(X_shape, n_vocab, embedding_dim=512) -> Sequential:
    lstm_model = Sequential()
    lstm_model.add(Embedding(input_dim=n_vocab, input_length=X_shape[0], output_dim=embedding_dim, mask_zero=True))
    lstm_model.add(LSTM(
        embedding_dim,
        return_sequences=True,
        activation='tanh', kernel_initializer='he_uniform', dropout=0.2
    ))
    lstm_model.add(Dropout(0.3))
    lstm_model.add(LSTM(490, return_sequences=True, activation='relu', dropout=0.2))
    lstm_model.add(Dropout(0.2))
    lstm_model.add(LSTM(2*n_vocab//3, activation='relu', dropout=0.2))
    lstm_model.add(Dense(n_vocab))
    lstm_model.add(Activation('softmax'))
    lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])
    return lstm_model


def train_model(lstm_model: Sequential, epochs=200, initial_epoch=0):
    # Set up callbacks
    # Set when to checkpoint
    filepath = "files/models/notes/model-{epoch:02d}-{loss:.4f}.hdf5"
    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=False, mode='min')

    # Set up live training plotting
    plot = PlotLearning('categorical_accuracy', 'categorical accuracy', 'models/notes/notes_logs.txt', 'models/notes/graph_notes')
    callbacks_list = [checkpoint, plot]

    # set up training plotting
    plot.on_train_begin()
    if glob.glob('files/models/notes/notes_logs.txt') and initial_epoch != 0:
        plot.load_in_data('files/models/notes/notes_logs.txt')

    train_batch_size = 1
    train_x_files = glob.glob('files/batch_data/train/x*')
    train_y_files = glob.glob('files/batch_data/train/y*')
    if len(train_x_files) != len(train_y_files):
        raise FileExistsError("The number of x and y values for training is not the same")
    my_training_batch_generator = My_Custom_Generator(train_x_files, train_y_files, train_batch_size)

    val_batch_size = 1
    val_x_files = glob.glob('files/batch_data/val/x*')
    val_y_files = glob.glob('files/batch_data/val/y*')
    if len(val_x_files) != len(val_y_files):
        raise FileExistsError("The number of x and y values for validation is not the same")
    my_validation_batch_generator = My_Custom_Generator(val_x_files, val_y_files, val_batch_size)

    lstm_model.fit_generator(generator=my_training_batch_generator,
                        steps_per_epoch=len(my_training_batch_generator),
                        epochs=epochs,
                        initial_epoch=initial_epoch,
                        validation_data=my_validation_batch_generator,
                        validation_steps=len(my_validation_batch_generator), callbacks=callbacks_list)

"""
def generate_music(l_model, starter_notes=30, save_file='test_output'):
    notes = load_notes()
    start = np.random.randint(0, len(X) - (starter_notes+1))
    int_to_note = dict((number, note) for number, note in enumerate(sorted(set(item for item in notes))))
    pattern = list(X[start])
    prediction_output = []
    # Run on some starter data
    for i in tqdm(range(30), desc='Seeding music generation'):
        prediction_input = np.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(n_vocab[0])
        prediction = l_model.predict(prediction_input, verbose=0)
        pattern = list(X[start+i])

    # generate 500 notes
    pattern = list(X[start])
    for note_index in tqdm(range(500), desc='Generating music'):
        prediction_input = np.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(n_vocab[0])
        prediction = l_model.predict(prediction_input, verbose=0)
        index = np.argmax(prediction)
        result = int_to_note[index]
        prediction_output.append(result)
        pattern.append(index)
        pattern = pattern[1:len(pattern)]

    offset = 0
    output_notes = []
    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)
        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp=save_file+'.mid')
"""

if __name__ == '__main__':
    option = get_user_options('What would you like to do', ['Train the model', 'Exit'])
    while option < 2:
        if option == 1:

            # create model

            n_vocab = len(read_pitchnames())

            model = create_model(read_size_of_data(), n_vocab)
            model.summary()
            print(model.input_shape)
            print(read_size_of_data(), read_pitchnames())
            print(n_vocab)

            if get_user_yes_no('Would you like to resume a training session'):
                start_epoch = int(get_user_non_negative_number('What epoch were you on'))
                end_epoch = int(get_user_non_negative_number('How many epochs would you like to train in total'))
                filename = get_user_filename("What is the model weight file")
                model.load_weights(filename)
                train_model(model, epochs=end_epoch, initial_epoch=start_epoch)
            else:
                end_epoch = int(get_user_non_negative_number('How many epochs would you like to run'))
                train_model(model, epochs=end_epoch)

        option = get_user_options('What would you like to do:',
                                  ['Train the model', 'Exit'])
