from keras import Sequential, optimizers
from keras.callbacks import ModelCheckpoint
from keras.layers import Activation, Dense, Dropout, LSTM, Flatten
from sklearn.model_selection import train_test_split

from Generator import My_Custom_Generator
from plot_losses import PlotLearning
from training_utils import *
from user_input import get_user_options, get_user_non_negative_number_or_default, get_user_yes_no, \
    get_user_non_negative_number, get_user_filename

from prep_batch_loading import read_size_of_data


def create_model_notes(X_shape) -> Sequential:
    # sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9,nesterov=True)
    lstm_model = Sequential()
    lstm_model.add(LSTM(
        256,
        input_shape=X_shape,
        return_sequences=True,
        activation='sigmoid', kernel_initializer='he_uniform'
    ))
    lstm_model.add(LSTM(256, activation='sigmoid', kernel_initializer='he_uniform'))
    lstm_model.add(Dense(256, activation='sigmoid', kernel_initializer='he_uniform'))
    lstm_model.add(Dropout(0.5))
    lstm_model.add(Dense(256, activation='sigmoid', kernel_initializer='he_uniform'))
    lstm_model.add(Dense(128, activation='sigmoid'))
    lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return lstm_model


def train_model_notes(lstm_model: Sequential, epochs=200, initial_epoch=0):
    # Set up callbacks
    # Set when to checkpoint
    filepath = "models/notes/note-model-{epoch:02d}-{loss:.4f}.hdf5"
    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')

    # Set up live training plotting
    plot = PlotLearning('accuracy', 'accuracy', 'models/notes/notes_logs.txt', 'models/notes/graph_notes')
    callbacks_list = [checkpoint, plot]

    # set up training plotting
    plot.on_train_begin()
    if glob.glob('models/notes/notes_logs.txt') and initial_epoch != 0:
        plot.load_in_data('models/notes/notes_logs.txt')

    train_batch_size = 1
    train_x_files = glob.glob('batch_data/notes/train/x*')
    train_y_files = glob.glob('batch_data/notes/train/y*')
    if len(train_x_files) != len(train_y_files):
        raise FileExistsError("The number of x and y values for training is not the same")
    my_training_batch_generator = My_Custom_Generator(train_x_files, train_y_files, train_batch_size)

    val_batch_size = 1
    val_x_files = glob.glob('batch_data/notes/val/x*')
    val_y_files = glob.glob('batch_data/notes/val/y*')
    if len(val_x_files) != len(val_y_files):
        raise FileExistsError("The number of x and y values for validation is not the same")
    my_validation_batch_generator = My_Custom_Generator(val_x_files, val_y_files, val_batch_size)

    lstm_model.fit_generator(generator=my_training_batch_generator,
                        steps_per_epoch=len(my_training_batch_generator),
                        epochs=epochs,
                        initial_epoch=initial_epoch,
                        verbose=1,
                        validation_data=my_validation_batch_generator,
                        validation_steps=len(my_validation_batch_generator), callbacks=callbacks_list)


"""
def generate_music(l_model, starter_notes=30, save_file='test_output'):
    notes = load_notes()
    start = np.random.randint(0, len(X) - (starter_notes+1))
    int_to_note = dict((number, note) for number, note in enumerate(sorted(set(item for item in notes))))
    pattern = list(X[start])
    prediction_output = []
    # Run on some starter data
    for i in tqdm(range(30), desc='Seeding music generation'):
        prediction_input = np.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(n_vocab[0])
        prediction = l_model.predict(prediction_input, verbose=0)
        pattern = list(X[start+i])

    # generate 500 notes
    pattern = list(X[start])
    for note_index in tqdm(range(500), desc='Generating music'):
        prediction_input = np.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(n_vocab[0])
        prediction = l_model.predict(prediction_input, verbose=0)
        index = np.argmax(prediction)
        result = int_to_note[index]
        prediction_output.append(result)
        pattern.append(index)
        pattern = pattern[1:len(pattern)]

    offset = 0
    output_notes = []
    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)
        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp=save_file+'.mid')
"""

if __name__ == '__main__':
    option = get_user_options('What would you like to do', ['Train the model', 'Exit'])
    while option < 2:
        if option == 1:

            # create model
            model = create_model_notes(read_size_of_data())
            model.summary()
            print(model.input_shape)

            if get_user_yes_no('Would you like to resume a training session'):
                start_epoch = int(get_user_non_negative_number('What epoch were you on'))
                end_epoch = int(get_user_non_negative_number('How many epochs would you like to train in total'))
                filename = get_user_filename("What is the model weight file")
                model.load_weights(filename)
                train_model_notes(model, epochs=end_epoch, initial_epoch=start_epoch)
            else:
                end_epoch = int(get_user_non_negative_number('How many epochs would you like to run'))
                train_model_notes(model, epochs=end_epoch)

        option = get_user_options('What would you like to do:',
                                  ['Train the model', 'Exit'])
